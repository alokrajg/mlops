INFO 2023-02-05 14:59:43,574 [root:main.py:elt_data:40]
✅ Saved data!

INFO 2023-02-05 19:14:13,594 [root:main.py:elt_data:40]
✅ Saved data!

INFO 2023-02-10 14:51:52,432 [root:main.py:elt_data:40]
✅ Saved data!

INFO 2023-02-10 14:54:43,717 [root:train.py:train:76]
Epoch: 00 | train_loss: 0.55909, val_loss: 0.67880

INFO 2023-02-10 14:54:43,769 [root:train.py:train:76]
Epoch: 10 | train_loss: 0.17090, val_loss: 0.40062

INFO 2023-02-10 14:54:43,819 [root:train.py:train:76]
Epoch: 20 | train_loss: 0.14768, val_loss: 0.38666

INFO 2023-02-10 14:54:43,868 [root:train.py:train:76]
Epoch: 30 | train_loss: 0.14321, val_loss: 0.38248

INFO 2023-02-10 14:54:43,919 [root:train.py:train:76]
Epoch: 40 | train_loss: 0.14173, val_loss: 0.38137

INFO 2023-02-10 14:54:43,968 [root:train.py:train:76]
Epoch: 50 | train_loss: 0.14297, val_loss: 0.38228

INFO 2023-02-10 14:54:44,018 [root:train.py:train:76]
Epoch: 60 | train_loss: 0.13852, val_loss: 0.38254

INFO 2023-02-10 14:54:44,069 [root:train.py:train:76]
Epoch: 70 | train_loss: 0.13954, val_loss: 0.38011

INFO 2023-02-10 14:54:44,119 [root:train.py:train:76]
Epoch: 80 | train_loss: 0.13935, val_loss: 0.37833

INFO 2023-02-10 14:54:44,168 [root:train.py:train:76]
Epoch: 90 | train_loss: 0.13722, val_loss: 0.38050

INFO 2023-02-10 14:54:44,228 [root:train.py:objective:134]
{
  "precision": 0.9189814814814815,
  "recall": 0.8055555555555556,
  "f1": 0.831809799634333,
  "num_samples": 144.0
}

INFO 2023-02-10 14:54:44,422 [root:train.py:train:76]
Epoch: 00 | train_loss: 1.31824, val_loss: 1.33037

INFO 2023-02-10 14:54:44,479 [root:train.py:train:76]
Epoch: 10 | train_loss: 0.82430, val_loss: 0.91553

INFO 2023-02-10 14:54:44,536 [root:train.py:train:76]
Epoch: 20 | train_loss: 0.62583, val_loss: 0.75092

INFO 2023-02-10 14:54:44,591 [root:train.py:train:76]
Epoch: 30 | train_loss: 0.51715, val_loss: 0.66356

INFO 2023-02-10 14:54:44,647 [root:train.py:train:76]
Epoch: 40 | train_loss: 0.44683, val_loss: 0.60863

INFO 2023-02-10 14:54:44,703 [root:train.py:train:76]
Epoch: 50 | train_loss: 0.39685, val_loss: 0.57071

INFO 2023-02-10 14:54:44,759 [root:train.py:train:76]
Epoch: 60 | train_loss: 0.35900, val_loss: 0.54262

INFO 2023-02-10 14:54:44,816 [root:train.py:train:76]
Epoch: 70 | train_loss: 0.32953, val_loss: 0.52114

INFO 2023-02-10 14:54:44,872 [root:train.py:train:76]
Epoch: 80 | train_loss: 0.30591, val_loss: 0.50417

INFO 2023-02-10 14:54:44,928 [root:train.py:train:76]
Epoch: 90 | train_loss: 0.28639, val_loss: 0.49046

INFO 2023-02-10 14:54:44,985 [root:train.py:objective:134]
{
  "precision": 0.8970125786163522,
  "recall": 0.75,
  "f1": 0.7841811437690531,
  "num_samples": 144.0
}

INFO 2023-02-10 14:54:45,049 [root:train.py:train:76]
Epoch: 00 | train_loss: 0.95229, val_loss: 1.09531

INFO 2023-02-10 14:54:45,073 [root:train.py:train:76]
Epoch: 10 | train_loss: 0.26852, val_loss: 0.71537

INFO 2023-02-10 14:54:45,096 [root:train.py:train:76]
Epoch: 20 | train_loss: 0.21367, val_loss: 0.68318

INFO 2023-02-10 14:54:45,119 [root:train.py:train:76]
Epoch: 30 | train_loss: 0.20093, val_loss: 0.67399

INFO 2023-02-10 14:54:45,142 [root:train.py:train:76]
Epoch: 40 | train_loss: 0.19684, val_loss: 0.67051

INFO 2023-02-10 14:54:45,165 [root:train.py:train:76]
Epoch: 50 | train_loss: 0.19516, val_loss: 0.66893

INFO 2023-02-10 14:54:45,188 [root:train.py:train:76]
Epoch: 60 | train_loss: 0.19419, val_loss: 0.66790

INFO 2023-02-10 14:54:45,211 [root:train.py:train:76]
Epoch: 70 | train_loss: 0.19381, val_loss: 0.66736

INFO 2023-02-10 14:54:45,234 [root:train.py:train:76]
Epoch: 80 | train_loss: 0.19356, val_loss: 0.66688

INFO 2023-02-10 14:54:45,257 [root:train.py:train:76]
Epoch: 90 | train_loss: 0.19307, val_loss: 0.66665

INFO 2023-02-10 14:54:45,283 [root:train.py:objective:134]
{
  "precision": 0.7973326276897706,
  "recall": 0.7083333333333334,
  "f1": 0.7315701102066794,
  "num_samples": 144.0
}

INFO 2023-02-10 14:54:45,463 [root:train.py:train:76]
Epoch: 00 | train_loss: 0.52792, val_loss: 0.67274

INFO 2023-02-10 14:54:45,535 [root:train.py:train:76]
Epoch: 10 | train_loss: 0.15981, val_loss: 0.40981

INFO 2023-02-10 14:54:45,608 [root:train.py:train:76]
Epoch: 20 | train_loss: 0.14370, val_loss: 0.39937

INFO 2023-02-10 14:54:45,681 [root:train.py:train:76]
Epoch: 30 | train_loss: 0.14076, val_loss: 0.39594

INFO 2023-02-10 14:54:45,755 [root:train.py:train:76]
Epoch: 40 | train_loss: 0.13923, val_loss: 0.39436

INFO 2023-02-10 14:54:45,828 [root:train.py:train:76]
Epoch: 50 | train_loss: 0.13961, val_loss: 0.39433

INFO 2023-02-10 14:54:45,902 [root:train.py:train:76]
Epoch: 60 | train_loss: 0.13581, val_loss: 0.39462

INFO 2023-02-10 14:54:45,975 [root:train.py:train:76]
Epoch: 70 | train_loss: 0.13648, val_loss: 0.39312

INFO 2023-02-10 14:54:46,048 [root:train.py:train:76]
Epoch: 80 | train_loss: 0.13602, val_loss: 0.39089

INFO 2023-02-10 14:54:46,121 [root:train.py:train:76]
Epoch: 90 | train_loss: 0.13399, val_loss: 0.39240

INFO 2023-02-10 14:54:46,193 [root:train.py:objective:134]
{
  "precision": 0.9155773420479303,
  "recall": 0.7847222222222222,
  "f1": 0.8144557069882609,
  "num_samples": 144.0
}

INFO 2023-02-10 14:54:46,389 [root:train.py:train:76]
Epoch: 00 | train_loss: 0.51273, val_loss: 0.64673

INFO 2023-02-10 14:54:46,463 [root:train.py:train:76]
Epoch: 10 | train_loss: 0.15939, val_loss: 0.39781

INFO 2023-02-10 14:54:46,537 [root:train.py:train:76]
Epoch: 20 | train_loss: 0.14352, val_loss: 0.38799

INFO 2023-02-10 14:54:46,611 [root:train.py:train:76]
Epoch: 30 | train_loss: 0.14032, val_loss: 0.38365

INFO 2023-02-10 14:54:46,686 [root:train.py:train:76]
Epoch: 40 | train_loss: 0.13899, val_loss: 0.38264

INFO 2023-02-10 14:54:46,760 [root:train.py:train:76]
Epoch: 50 | train_loss: 0.13943, val_loss: 0.38319

INFO 2023-02-10 14:54:46,834 [root:train.py:train:76]
Epoch: 60 | train_loss: 0.13535, val_loss: 0.38306

INFO 2023-02-10 14:54:46,909 [root:train.py:train:76]
Epoch: 70 | train_loss: 0.13613, val_loss: 0.38118

INFO 2023-02-10 14:54:46,984 [root:train.py:train:76]
Epoch: 80 | train_loss: 0.13559, val_loss: 0.37886

INFO 2023-02-10 14:54:47,060 [root:train.py:train:76]
Epoch: 90 | train_loss: 0.13367, val_loss: 0.38066

INFO 2023-02-10 14:54:47,133 [root:train.py:objective:134]
{
  "precision": 0.9178004535147392,
  "recall": 0.7986111111111112,
  "f1": 0.8265063522446623,
  "num_samples": 144.0
}

INFO 2023-02-10 14:54:47,231 [root:train.py:train:76]
Epoch: 00 | train_loss: 1.06333, val_loss: 1.11333

INFO 2023-02-10 14:54:47,272 [root:train.py:train:76]
Epoch: 10 | train_loss: 0.41646, val_loss: 0.58203

INFO 2023-02-10 14:54:47,313 [root:train.py:train:76]
Epoch: 20 | train_loss: 0.29540, val_loss: 0.49653

INFO 2023-02-10 14:54:47,354 [root:train.py:train:76]
Epoch: 30 | train_loss: 0.24222, val_loss: 0.46207

INFO 2023-02-10 14:54:47,395 [root:train.py:train:76]
Epoch: 40 | train_loss: 0.21334, val_loss: 0.44420

INFO 2023-02-10 14:54:47,436 [root:train.py:train:76]
Epoch: 50 | train_loss: 0.19528, val_loss: 0.43368

INFO 2023-02-10 14:54:47,477 [root:train.py:train:76]
Epoch: 60 | train_loss: 0.18257, val_loss: 0.42732

INFO 2023-02-10 14:54:47,518 [root:train.py:train:76]
Epoch: 70 | train_loss: 0.17439, val_loss: 0.42222

INFO 2023-02-10 14:54:47,559 [root:train.py:train:76]
Epoch: 80 | train_loss: 0.16882, val_loss: 0.41879

INFO 2023-02-10 14:54:47,601 [root:train.py:train:76]
Epoch: 90 | train_loss: 0.16417, val_loss: 0.41718

INFO 2023-02-10 14:54:47,643 [root:train.py:objective:134]
{
  "precision": 0.8902091029648574,
  "recall": 0.7569444444444444,
  "f1": 0.7872808552554161,
  "num_samples": 144.0
}

INFO 2023-02-10 14:54:47,707 [root:train.py:train:76]
Epoch: 00 | train_loss: 1.26616, val_loss: 1.29665

INFO 2023-02-10 14:54:47,731 [root:train.py:train:76]
Epoch: 10 | train_loss: 0.60112, val_loss: 0.89673

INFO 2023-02-10 14:54:47,755 [root:train.py:train:76]
Epoch: 20 | train_loss: 0.40756, val_loss: 0.79146

INFO 2023-02-10 14:54:47,779 [root:train.py:train:76]
Epoch: 30 | train_loss: 0.32364, val_loss: 0.74588

INFO 2023-02-10 14:54:47,802 [root:train.py:train:76]
Epoch: 40 | train_loss: 0.27896, val_loss: 0.72123

INFO 2023-02-10 14:54:47,825 [root:train.py:train:76]
Epoch: 50 | train_loss: 0.25246, val_loss: 0.70623

INFO 2023-02-10 14:54:47,849 [root:train.py:train:76]
Epoch: 60 | train_loss: 0.23562, val_loss: 0.69640

INFO 2023-02-10 14:54:47,873 [root:train.py:train:76]
Epoch: 70 | train_loss: 0.22451, val_loss: 0.68967

INFO 2023-02-10 14:54:47,897 [root:train.py:train:76]
Epoch: 80 | train_loss: 0.21691, val_loss: 0.68486

INFO 2023-02-10 14:54:47,920 [root:train.py:train:76]
Epoch: 90 | train_loss: 0.21154, val_loss: 0.68136

INFO 2023-02-10 14:54:47,946 [root:train.py:objective:134]
{
  "precision": 0.7973326276897706,
  "recall": 0.7083333333333334,
  "f1": 0.7315701102066794,
  "num_samples": 144.0
}

INFO 2023-02-10 14:54:48,140 [root:train.py:train:76]
Epoch: 00 | train_loss: 1.27933, val_loss: 1.29553

INFO 2023-02-10 14:54:48,215 [root:train.py:train:76]
Epoch: 10 | train_loss: 0.69146, val_loss: 0.79344

INFO 2023-02-10 14:54:48,291 [root:train.py:train:76]
Epoch: 20 | train_loss: 0.50668, val_loss: 0.64386

INFO 2023-02-10 14:54:48,366 [root:train.py:train:76]
Epoch: 30 | train_loss: 0.41150, val_loss: 0.57084

INFO 2023-02-10 14:54:48,441 [root:train.py:train:76]
Epoch: 40 | train_loss: 0.35216, val_loss: 0.52702

INFO 2023-02-10 14:54:48,515 [root:train.py:train:76]
Epoch: 50 | train_loss: 0.31128, val_loss: 0.49787

INFO 2023-02-10 14:54:48,590 [root:train.py:train:76]
Epoch: 60 | train_loss: 0.28124, val_loss: 0.47689

INFO 2023-02-10 14:54:48,665 [root:train.py:train:76]
Epoch: 70 | train_loss: 0.25847, val_loss: 0.46124

INFO 2023-02-10 14:54:48,740 [root:train.py:train:76]
Epoch: 80 | train_loss: 0.24072, val_loss: 0.44916

INFO 2023-02-10 14:54:48,814 [root:train.py:train:76]
Epoch: 90 | train_loss: 0.22640, val_loss: 0.43964

INFO 2023-02-10 14:54:48,888 [root:train.py:objective:134]
{
  "precision": 0.9145299145299144,
  "recall": 0.7777777777777778,
  "f1": 0.8098048721026068,
  "num_samples": 144.0
}

INFO 2023-02-10 14:54:49,066 [root:train.py:train:76]
Epoch: 00 | train_loss: 0.43967, val_loss: 0.58700

INFO 2023-02-10 14:54:49,478 [root:train.py:train:76]
Epoch: 00 | train_loss: 1.14129, val_loss: 1.17701

INFO 2023-02-10 14:54:49,616 [root:train.py:train:76]
Epoch: 10 | train_loss: 0.42624, val_loss: 0.59929

INFO 2023-02-10 14:54:49,755 [root:train.py:train:76]
Epoch: 20 | train_loss: 0.28958, val_loss: 0.50329

INFO 2023-02-10 14:54:49,893 [root:train.py:train:76]
Epoch: 30 | train_loss: 0.23254, val_loss: 0.46476

INFO 2023-02-10 14:54:50,032 [root:train.py:train:76]
Epoch: 40 | train_loss: 0.20285, val_loss: 0.44495

INFO 2023-02-10 14:54:50,172 [root:train.py:train:76]
Epoch: 50 | train_loss: 0.18543, val_loss: 0.43365

INFO 2023-02-10 14:54:50,315 [root:train.py:train:76]
Epoch: 60 | train_loss: 0.17431, val_loss: 0.42660

INFO 2023-02-10 14:54:50,455 [root:train.py:train:76]
Epoch: 70 | train_loss: 0.16711, val_loss: 0.42205

INFO 2023-02-10 14:54:50,595 [root:train.py:train:76]
Epoch: 80 | train_loss: 0.16224, val_loss: 0.41896

INFO 2023-02-10 14:54:50,743 [root:train.py:train:76]
Epoch: 90 | train_loss: 0.15864, val_loss: 0.41702

INFO 2023-02-10 14:54:50,879 [root:train.py:objective:134]
{
  "precision": 0.9023569023569024,
  "recall": 0.75,
  "f1": 0.785060690943044,
  "num_samples": 144.0
}

INFO 2023-02-10 14:54:50,894 [root:main.py:optimize:121]

Best value (f1): 0.831809799634333

INFO 2023-02-10 14:54:50,895 [root:main.py:optimize:122]
Best hyperparameters: {
  "analyzer": "char_wb",
  "ngram_max_range": 4,
  "learning_rate": 0.5735762483144123,
  "power_t": 0.25483117443036507
}

INFO 2023-02-10 14:56:02,691 [root:main.py:train_model:65]
Run ID: 03fa635e88b94b9bad0ffb6fa94c6de2

INFO 2023-02-10 14:56:02,819 [root:train.py:train:76]
Epoch: 00 | train_loss: 0.55909, val_loss: 0.67880

INFO 2023-02-10 14:56:02,875 [root:train.py:train:76]
Epoch: 10 | train_loss: 0.17090, val_loss: 0.40062

INFO 2023-02-10 14:56:02,931 [root:train.py:train:76]
Epoch: 20 | train_loss: 0.14768, val_loss: 0.38666

INFO 2023-02-10 14:56:02,986 [root:train.py:train:76]
Epoch: 30 | train_loss: 0.14321, val_loss: 0.38248

INFO 2023-02-10 14:56:03,041 [root:train.py:train:76]
Epoch: 40 | train_loss: 0.14173, val_loss: 0.38137

INFO 2023-02-10 14:56:03,098 [root:train.py:train:76]
Epoch: 50 | train_loss: 0.14297, val_loss: 0.38228

INFO 2023-02-10 14:56:03,153 [root:train.py:train:76]
Epoch: 60 | train_loss: 0.13852, val_loss: 0.38254

INFO 2023-02-10 14:56:03,208 [root:train.py:train:76]
Epoch: 70 | train_loss: 0.13954, val_loss: 0.38011

INFO 2023-02-10 14:56:03,263 [root:train.py:train:76]
Epoch: 80 | train_loss: 0.13935, val_loss: 0.37833

INFO 2023-02-10 14:56:03,318 [root:train.py:train:76]
Epoch: 90 | train_loss: 0.13722, val_loss: 0.38050

INFO 2023-02-10 14:56:03,382 [root:main.py:train_model:68]
{
  "overall": {
    "precision": 0.9189814814814815,
    "recall": 0.8055555555555556,
    "f1": 0.831809799634333,
    "num_samples": 144.0
  },
  "class": {
    "computer-vision": {
      "precision": 1.0,
      "recall": 0.7037037037037037,
      "f1": 0.8260869565217391,
      "num_samples": 54.0
    },
    "mlops": {
      "precision": 1.0,
      "recall": 0.75,
      "f1": 0.8571428571428571,
      "num_samples": 12.0
    },
    "natural-language-processing": {
      "precision": 1.0,
      "recall": 0.8448275862068966,
      "f1": 0.9158878504672897,
      "num_samples": 58.0
    },
    "other": {
      "precision": 0.4166666666666667,
      "recall": 1.0,
      "f1": 0.5882352941176471,
      "num_samples": 20.0
    }
  },
  "slices": {
    "nlp_cnn": {
      "precision": 1.0,
      "recall": 1.0,
      "f1": 1.0,
      "num_samples": 1
    },
    "short_text": {
      "precision": 0.8,
      "recall": 0.8,
      "f1": 0.8000000000000002,
      "num_samples": 5
    }
  }
}

INFO 2023-02-10 14:56:03,420 [root:file_util.py:copy_file:137]
copying /var/folders/6w/rwxjjfl12yg5rxnfzhg2rrp40000gn/T/tmp72xbqt9b/args.json -> /Users/alokrajgupta/ml-web/mlops/stores/model/762791503755819567/03fa635e88b94b9bad0ffb6fa94c6de2/artifacts

INFO 2023-02-10 14:56:03,421 [root:file_util.py:copy_file:137]
copying /var/folders/6w/rwxjjfl12yg5rxnfzhg2rrp40000gn/T/tmp72xbqt9b/model.pkl -> /Users/alokrajgupta/ml-web/mlops/stores/model/762791503755819567/03fa635e88b94b9bad0ffb6fa94c6de2/artifacts

INFO 2023-02-10 14:56:03,421 [root:file_util.py:copy_file:137]
copying /var/folders/6w/rwxjjfl12yg5rxnfzhg2rrp40000gn/T/tmp72xbqt9b/performance.json -> /Users/alokrajgupta/ml-web/mlops/stores/model/762791503755819567/03fa635e88b94b9bad0ffb6fa94c6de2/artifacts

INFO 2023-02-10 14:56:03,422 [root:file_util.py:copy_file:137]
copying /var/folders/6w/rwxjjfl12yg5rxnfzhg2rrp40000gn/T/tmp72xbqt9b/label_encoder.json -> /Users/alokrajgupta/ml-web/mlops/stores/model/762791503755819567/03fa635e88b94b9bad0ffb6fa94c6de2/artifacts

INFO 2023-02-10 14:56:03,423 [root:file_util.py:copy_file:137]
copying /var/folders/6w/rwxjjfl12yg5rxnfzhg2rrp40000gn/T/tmp72xbqt9b/vectorizer.pkl -> /Users/alokrajgupta/ml-web/mlops/stores/model/762791503755819567/03fa635e88b94b9bad0ffb6fa94c6de2/artifacts

INFO 2023-02-10 14:56:37,983 [root:main.py:train_model:65]
Run ID: d4d3d2c7e5e8476b8db8963599d06b59

INFO 2023-02-10 14:56:38,110 [root:train.py:train:76]
Epoch: 00 | train_loss: 0.55909, val_loss: 0.67880

INFO 2023-02-10 14:56:38,166 [root:train.py:train:76]
Epoch: 10 | train_loss: 0.17090, val_loss: 0.40062

INFO 2023-02-10 14:56:38,222 [root:train.py:train:76]
Epoch: 20 | train_loss: 0.14768, val_loss: 0.38666

INFO 2023-02-10 14:56:38,277 [root:train.py:train:76]
Epoch: 30 | train_loss: 0.14321, val_loss: 0.38248

INFO 2023-02-10 14:56:38,334 [root:train.py:train:76]
Epoch: 40 | train_loss: 0.14173, val_loss: 0.38137

INFO 2023-02-10 14:56:38,390 [root:train.py:train:76]
Epoch: 50 | train_loss: 0.14297, val_loss: 0.38228

INFO 2023-02-10 14:56:38,444 [root:train.py:train:76]
Epoch: 60 | train_loss: 0.13852, val_loss: 0.38254

INFO 2023-02-10 14:56:38,500 [root:train.py:train:76]
Epoch: 70 | train_loss: 0.13954, val_loss: 0.38011

INFO 2023-02-10 14:56:38,555 [root:train.py:train:76]
Epoch: 80 | train_loss: 0.13935, val_loss: 0.37833

INFO 2023-02-10 14:56:38,610 [root:train.py:train:76]
Epoch: 90 | train_loss: 0.13722, val_loss: 0.38050

INFO 2023-02-10 14:56:38,674 [root:main.py:train_model:68]
{
  "overall": {
    "precision": 0.9189814814814815,
    "recall": 0.8055555555555556,
    "f1": 0.831809799634333,
    "num_samples": 144.0
  },
  "class": {
    "computer-vision": {
      "precision": 1.0,
      "recall": 0.7037037037037037,
      "f1": 0.8260869565217391,
      "num_samples": 54.0
    },
    "mlops": {
      "precision": 1.0,
      "recall": 0.75,
      "f1": 0.8571428571428571,
      "num_samples": 12.0
    },
    "natural-language-processing": {
      "precision": 1.0,
      "recall": 0.8448275862068966,
      "f1": 0.9158878504672897,
      "num_samples": 58.0
    },
    "other": {
      "precision": 0.4166666666666667,
      "recall": 1.0,
      "f1": 0.5882352941176471,
      "num_samples": 20.0
    }
  },
  "slices": {
    "nlp_cnn": {
      "precision": 1.0,
      "recall": 1.0,
      "f1": 1.0,
      "num_samples": 1
    },
    "short_text": {
      "precision": 0.8,
      "recall": 0.8,
      "f1": 0.8000000000000002,
      "num_samples": 5
    }
  }
}

INFO 2023-02-10 14:56:38,711 [root:file_util.py:copy_file:137]
copying /var/folders/6w/rwxjjfl12yg5rxnfzhg2rrp40000gn/T/tmpz6szhstm/args.json -> /Users/alokrajgupta/ml-web/mlops/stores/model/762791503755819567/d4d3d2c7e5e8476b8db8963599d06b59/artifacts

INFO 2023-02-10 14:56:38,712 [root:file_util.py:copy_file:137]
copying /var/folders/6w/rwxjjfl12yg5rxnfzhg2rrp40000gn/T/tmpz6szhstm/model.pkl -> /Users/alokrajgupta/ml-web/mlops/stores/model/762791503755819567/d4d3d2c7e5e8476b8db8963599d06b59/artifacts

INFO 2023-02-10 14:56:38,713 [root:file_util.py:copy_file:137]
copying /var/folders/6w/rwxjjfl12yg5rxnfzhg2rrp40000gn/T/tmpz6szhstm/performance.json -> /Users/alokrajgupta/ml-web/mlops/stores/model/762791503755819567/d4d3d2c7e5e8476b8db8963599d06b59/artifacts

INFO 2023-02-10 14:56:38,714 [root:file_util.py:copy_file:137]
copying /var/folders/6w/rwxjjfl12yg5rxnfzhg2rrp40000gn/T/tmpz6szhstm/label_encoder.json -> /Users/alokrajgupta/ml-web/mlops/stores/model/762791503755819567/d4d3d2c7e5e8476b8db8963599d06b59/artifacts

INFO 2023-02-10 14:56:38,714 [root:file_util.py:copy_file:137]
copying /var/folders/6w/rwxjjfl12yg5rxnfzhg2rrp40000gn/T/tmpz6szhstm/vectorizer.pkl -> /Users/alokrajgupta/ml-web/mlops/stores/model/762791503755819567/d4d3d2c7e5e8476b8db8963599d06b59/artifacts

INFO 2023-02-10 14:57:01,901 [root:api.py:load_artifacts:28]
Ready for inference!

INFO 2023-02-10 14:59:13,326 [root:api.py:load_artifacts:28]
Ready for inference!

INFO 2023-02-10 14:59:34,587 [root:api.py:load_artifacts:28]
Ready for inference!

INFO 2023-02-10 14:59:37,019 [root:api.py:load_artifacts:28]
Ready for inference!

INFO 2023-02-10 14:59:39,573 [root:api.py:load_artifacts:28]
Ready for inference!

INFO 2023-02-10 15:02:47,107 [root:api.py:load_artifacts:28]
Ready for inference!

INFO 2023-02-10 15:05:21,942 [root:api.py:load_artifacts:28]
Ready for inference!

INFO 2023-02-11 11:53:49,289 [root:main.py:elt_data:40]
✅ Saved data!

INFO 2023-02-11 11:53:49,503 [root:main.py:predict_tag:166]
[
  {
    "input_text": "Transfer learning with transformers for text classification.",
    "predicted_tag": "natural-language-processing"
  }
]

INFO 2023-02-11 11:55:11,565 [root:main.py:elt_data:40]
✅ Saved data!

INFO 2023-02-11 11:55:11,788 [root:main.py:predict_tag:166]
[
  {
    "input_text": "Transfer learning with transformers for text classification.",
    "predicted_tag": "natural-language-processing"
  }
]

INFO 2023-02-11 11:55:57,981 [root:train.py:train:76]
Epoch: 00 | train_loss: 0.52732, val_loss: 0.66937

INFO 2023-02-11 11:55:58,146 [root:train.py:train:76]
Epoch: 10 | train_loss: 0.16341, val_loss: 0.42403

INFO 2023-02-11 11:55:58,308 [root:train.py:train:76]
Epoch: 20 | train_loss: 0.15059, val_loss: 0.41739

INFO 2023-02-11 11:55:58,469 [root:train.py:train:76]
Epoch: 30 | train_loss: 0.14744, val_loss: 0.41527

INFO 2023-02-11 11:55:58,640 [root:train.py:train:76]
Epoch: 40 | train_loss: 0.14553, val_loss: 0.41440

INFO 2023-02-11 11:55:58,803 [root:train.py:train:76]
Epoch: 50 | train_loss: 0.14481, val_loss: 0.41462

INFO 2023-02-11 11:55:58,964 [root:train.py:train:76]
Epoch: 60 | train_loss: 0.14211, val_loss: 0.41512

INFO 2023-02-11 11:55:59,127 [root:train.py:train:76]
Epoch: 70 | train_loss: 0.14242, val_loss: 0.41465

INFO 2023-02-11 11:55:59,286 [root:train.py:train:76]
Epoch: 80 | train_loss: 0.14204, val_loss: 0.41310

INFO 2023-02-11 11:55:59,447 [root:train.py:train:76]
Epoch: 90 | train_loss: 0.14040, val_loss: 0.41430

INFO 2023-02-11 11:55:59,614 [root:train.py:objective:134]
{
  "precision": 0.9023569023569024,
  "recall": 0.75,
  "f1": 0.785060690943044,
  "num_samples": 144.0
}

INFO 2023-02-11 11:55:59,773 [root:train.py:train:76]
Epoch: 00 | train_loss: 1.06240, val_loss: 1.11254

INFO 2023-02-11 11:55:59,815 [root:train.py:train:76]
Epoch: 10 | train_loss: 0.41569, val_loss: 0.58146

INFO 2023-02-11 11:55:59,857 [root:train.py:train:76]
Epoch: 20 | train_loss: 0.29483, val_loss: 0.49616

INFO 2023-02-11 11:55:59,897 [root:train.py:train:76]
Epoch: 30 | train_loss: 0.24179, val_loss: 0.46180

INFO 2023-02-11 11:55:59,938 [root:train.py:train:76]
Epoch: 40 | train_loss: 0.21300, val_loss: 0.44399

INFO 2023-02-11 11:55:59,978 [root:train.py:train:76]
Epoch: 50 | train_loss: 0.19501, val_loss: 0.43352

INFO 2023-02-11 11:56:00,018 [root:train.py:train:76]
Epoch: 60 | train_loss: 0.18235, val_loss: 0.42720

INFO 2023-02-11 11:56:00,059 [root:train.py:train:76]
Epoch: 70 | train_loss: 0.17421, val_loss: 0.42213

INFO 2023-02-11 11:56:00,099 [root:train.py:train:76]
Epoch: 80 | train_loss: 0.16867, val_loss: 0.41872

INFO 2023-02-11 11:56:00,140 [root:train.py:train:76]
Epoch: 90 | train_loss: 0.16405, val_loss: 0.41712

INFO 2023-02-11 11:56:00,181 [root:train.py:objective:134]
{
  "precision": 0.8902091029648574,
  "recall": 0.7569444444444444,
  "f1": 0.7872808552554161,
  "num_samples": 144.0
}

INFO 2023-02-11 11:56:00,254 [root:train.py:train:76]
Epoch: 00 | train_loss: 1.36176, val_loss: 1.36911

INFO 2023-02-11 11:56:00,281 [root:train.py:train:76]
Epoch: 10 | train_loss: 1.11268, val_loss: 1.20632

INFO 2023-02-11 11:56:00,307 [root:train.py:train:76]
Epoch: 20 | train_loss: 0.91917, val_loss: 1.09192

INFO 2023-02-11 11:56:00,333 [root:train.py:train:76]
Epoch: 30 | train_loss: 0.77879, val_loss: 1.01222

INFO 2023-02-11 11:56:00,359 [root:train.py:train:76]
Epoch: 40 | train_loss: 0.67583, val_loss: 0.95476

INFO 2023-02-11 11:56:00,385 [root:train.py:train:76]
Epoch: 50 | train_loss: 0.59848, val_loss: 0.91194

INFO 2023-02-11 11:56:00,411 [root:train.py:train:76]
Epoch: 60 | train_loss: 0.53883, val_loss: 0.87907

INFO 2023-02-11 11:56:00,436 [root:train.py:train:76]
Epoch: 70 | train_loss: 0.49175, val_loss: 0.85319

INFO 2023-02-11 11:56:00,462 [root:train.py:train:76]
Epoch: 80 | train_loss: 0.45381, val_loss: 0.83236

INFO 2023-02-11 11:56:00,488 [root:train.py:train:76]
Epoch: 90 | train_loss: 0.42269, val_loss: 0.81528

INFO 2023-02-11 11:56:00,516 [root:train.py:objective:134]
{
  "precision": 0.8006086541800826,
  "recall": 0.6805555555555556,
  "f1": 0.7115358913712648,
  "num_samples": 144.0
}

INFO 2023-02-11 11:56:00,648 [root:train.py:train:76]
Epoch: 00 | train_loss: 1.05181, val_loss: 1.09943

INFO 2023-02-11 11:56:00,698 [root:train.py:train:76]
Epoch: 10 | train_loss: 0.38984, val_loss: 0.54849

INFO 2023-02-11 11:56:00,748 [root:train.py:train:76]
Epoch: 20 | train_loss: 0.27286, val_loss: 0.46445

INFO 2023-02-11 11:56:00,797 [root:train.py:train:76]
Epoch: 30 | train_loss: 0.22266, val_loss: 0.43084

INFO 2023-02-11 11:56:00,847 [root:train.py:train:76]
Epoch: 40 | train_loss: 0.19608, val_loss: 0.41373

INFO 2023-02-11 11:56:00,897 [root:train.py:train:76]
Epoch: 50 | train_loss: 0.17979, val_loss: 0.40395

INFO 2023-02-11 11:56:00,947 [root:train.py:train:76]
Epoch: 60 | train_loss: 0.16880, val_loss: 0.39784

INFO 2023-02-11 11:56:00,997 [root:train.py:train:76]
Epoch: 70 | train_loss: 0.16168, val_loss: 0.39334

INFO 2023-02-11 11:56:01,047 [root:train.py:train:76]
Epoch: 80 | train_loss: 0.15694, val_loss: 0.39037

INFO 2023-02-11 11:56:01,098 [root:train.py:train:76]
Epoch: 90 | train_loss: 0.15315, val_loss: 0.38886

INFO 2023-02-11 11:56:01,148 [root:train.py:objective:134]
{
  "precision": 0.9178004535147392,
  "recall": 0.7986111111111112,
  "f1": 0.8265063522446623,
  "num_samples": 144.0
}

INFO 2023-02-11 11:56:01,370 [root:train.py:train:76]
Epoch: 00 | train_loss: 1.35682, val_loss: 1.36149

INFO 2023-02-11 11:56:01,455 [root:train.py:train:76]
Epoch: 10 | train_loss: 1.06574, val_loss: 1.11168

INFO 2023-02-11 11:56:01,541 [root:train.py:train:76]
Epoch: 20 | train_loss: 0.87688, val_loss: 0.94965

INFO 2023-02-11 11:56:01,625 [root:train.py:train:76]
Epoch: 30 | train_loss: 0.75415, val_loss: 0.84586

INFO 2023-02-11 11:56:01,710 [root:train.py:train:76]
Epoch: 40 | train_loss: 0.66800, val_loss: 0.77449

INFO 2023-02-11 11:56:01,795 [root:train.py:train:76]
Epoch: 50 | train_loss: 0.60353, val_loss: 0.72224

INFO 2023-02-11 11:56:01,879 [root:train.py:train:76]
Epoch: 60 | train_loss: 0.55295, val_loss: 0.68205

INFO 2023-02-11 11:56:01,963 [root:train.py:train:76]
Epoch: 70 | train_loss: 0.51199, val_loss: 0.65010

INFO 2023-02-11 11:56:02,048 [root:train.py:train:76]
Epoch: 80 | train_loss: 0.47799, val_loss: 0.62401

INFO 2023-02-11 11:56:02,133 [root:train.py:train:76]
Epoch: 90 | train_loss: 0.44917, val_loss: 0.60226

INFO 2023-02-11 11:56:02,216 [root:train.py:objective:134]
{
  "precision": 0.886893539467069,
  "recall": 0.75,
  "f1": 0.782917088588174,
  "num_samples": 144.0
}

INFO 2023-02-11 11:56:02,803 [root:train.py:train:76]
Epoch: 00 | train_loss: 1.03033, val_loss: 1.07717

INFO 2023-02-11 11:56:02,992 [root:train.py:train:76]
Epoch: 10 | train_loss: 0.32328, val_loss: 0.52887

INFO 2023-02-11 11:56:03,180 [root:train.py:train:76]
Epoch: 20 | train_loss: 0.22427, val_loss: 0.46396

INFO 2023-02-11 11:56:03,368 [root:train.py:train:76]
Epoch: 30 | train_loss: 0.18929, val_loss: 0.44163

INFO 2023-02-11 11:56:03,556 [root:train.py:train:76]
Epoch: 40 | train_loss: 0.17356, val_loss: 0.43186

INFO 2023-02-11 11:56:03,747 [root:train.py:train:76]
Epoch: 50 | train_loss: 0.16540, val_loss: 0.42732

INFO 2023-02-11 11:56:03,935 [root:train.py:train:76]
Epoch: 60 | train_loss: 0.16050, val_loss: 0.42495

INFO 2023-02-11 11:56:04,121 [root:train.py:train:76]
Epoch: 70 | train_loss: 0.15766, val_loss: 0.42364

INFO 2023-02-11 11:56:04,309 [root:train.py:train:76]
Epoch: 80 | train_loss: 0.15580, val_loss: 0.42274

INFO 2023-02-11 11:56:04,496 [root:train.py:train:76]
Epoch: 90 | train_loss: 0.15423, val_loss: 0.42251

INFO 2023-02-11 11:56:04,677 [root:train.py:objective:134]
{
  "precision": 0.9014550264550265,
  "recall": 0.7430555555555556,
  "f1": 0.7795507746263348,
  "num_samples": 144.0
}

INFO 2023-02-11 11:56:04,765 [root:train.py:train:76]
Epoch: 00 | train_loss: 1.35435, val_loss: 1.36545

INFO 2023-02-11 11:56:04,794 [root:train.py:train:76]
Epoch: 10 | train_loss: 1.04101, val_loss: 1.17287

INFO 2023-02-11 11:56:04,823 [root:train.py:train:76]
Epoch: 20 | train_loss: 0.82361, val_loss: 1.04901

INFO 2023-02-11 11:56:04,853 [root:train.py:train:76]
Epoch: 30 | train_loss: 0.67975, val_loss: 0.96873

INFO 2023-02-11 11:56:04,882 [root:train.py:train:76]
Epoch: 40 | train_loss: 0.58114, val_loss: 0.91405

INFO 2023-02-11 11:56:04,911 [root:train.py:train:76]
Epoch: 50 | train_loss: 0.51055, val_loss: 0.87498

INFO 2023-02-11 11:56:04,940 [root:train.py:train:76]
Epoch: 60 | train_loss: 0.45799, val_loss: 0.84588

INFO 2023-02-11 11:56:04,969 [root:train.py:train:76]
Epoch: 70 | train_loss: 0.41763, val_loss: 0.82350

INFO 2023-02-11 11:56:04,998 [root:train.py:train:76]
Epoch: 80 | train_loss: 0.38585, val_loss: 0.80580

INFO 2023-02-11 11:56:05,027 [root:train.py:train:76]
Epoch: 90 | train_loss: 0.36031, val_loss: 0.79153

INFO 2023-02-11 11:56:05,058 [root:train.py:objective:134]
{
  "precision": 0.79786568483377,
  "recall": 0.6875,
  "f1": 0.715842790107496,
  "num_samples": 144.0
}

INFO 2023-02-11 11:56:05,179 [root:train.py:train:76]
Epoch: 00 | train_loss: 1.01552, val_loss: 1.18154

INFO 2023-02-11 11:56:05,218 [root:train.py:train:76]
Epoch: 10 | train_loss: 0.29946, val_loss: 0.78344

INFO 2023-02-11 11:56:05,257 [root:train.py:train:76]
Epoch: 20 | train_loss: 0.23518, val_loss: 0.74396

INFO 2023-02-11 11:56:05,297 [root:train.py:train:76]
Epoch: 30 | train_loss: 0.21998, val_loss: 0.73288

INFO 2023-02-11 11:56:05,337 [root:train.py:train:76]
Epoch: 40 | train_loss: 0.21519, val_loss: 0.72889

INFO 2023-02-11 11:56:05,377 [root:train.py:train:76]
Epoch: 50 | train_loss: 0.21338, val_loss: 0.72706

INFO 2023-02-11 11:56:05,417 [root:train.py:train:76]
Epoch: 60 | train_loss: 0.21243, val_loss: 0.72590

INFO 2023-02-11 11:56:05,456 [root:train.py:train:76]
Epoch: 70 | train_loss: 0.21210, val_loss: 0.72529

INFO 2023-02-11 11:56:05,497 [root:train.py:train:76]
Epoch: 80 | train_loss: 0.21186, val_loss: 0.72484

INFO 2023-02-11 11:56:05,537 [root:train.py:train:76]
Epoch: 90 | train_loss: 0.21140, val_loss: 0.72457

INFO 2023-02-11 11:56:05,579 [root:train.py:objective:134]
{
  "precision": 0.811613475177305,
  "recall": 0.7083333333333334,
  "f1": 0.7364935171858006,
  "num_samples": 144.0
}

INFO 2023-02-11 11:56:05,680 [root:train.py:train:76]
Epoch: 00 | train_loss: 1.34872, val_loss: 1.36397

INFO 2023-02-11 11:56:05,714 [root:train.py:train:76]
Epoch: 10 | train_loss: 0.99066, val_loss: 1.15753

INFO 2023-02-11 11:56:05,747 [root:train.py:train:76]
Epoch: 20 | train_loss: 0.76218, val_loss: 1.03065

INFO 2023-02-11 11:56:05,780 [root:train.py:train:76]
Epoch: 30 | train_loss: 0.62018, val_loss: 0.95194

INFO 2023-02-11 11:56:05,812 [root:train.py:train:76]
Epoch: 40 | train_loss: 0.52683, val_loss: 0.90005

INFO 2023-02-11 11:56:05,846 [root:train.py:train:76]
Epoch: 50 | train_loss: 0.46185, val_loss: 0.86378

INFO 2023-02-11 11:56:05,879 [root:train.py:train:76]
Epoch: 60 | train_loss: 0.41447, val_loss: 0.83718

INFO 2023-02-11 11:56:05,912 [root:train.py:train:76]
Epoch: 70 | train_loss: 0.37873, val_loss: 0.81697

INFO 2023-02-11 11:56:05,946 [root:train.py:train:76]
Epoch: 80 | train_loss: 0.35102, val_loss: 0.80119

INFO 2023-02-11 11:56:05,979 [root:train.py:train:76]
Epoch: 90 | train_loss: 0.32906, val_loss: 0.78858

INFO 2023-02-11 11:56:06,015 [root:train.py:objective:134]
{
  "precision": 0.799564857410602,
  "recall": 0.6944444444444444,
  "f1": 0.7213056761599426,
  "num_samples": 144.0
}

INFO 2023-02-11 11:56:06,152 [root:train.py:train:76]
Epoch: 00 | train_loss: 1.15352, val_loss: 1.19250

INFO 2023-02-11 11:56:06,209 [root:train.py:train:76]
Epoch: 10 | train_loss: 0.47555, val_loss: 0.63114

INFO 2023-02-11 11:56:06,266 [root:train.py:train:76]
Epoch: 20 | train_loss: 0.33256, val_loss: 0.52344

INFO 2023-02-11 11:56:06,322 [root:train.py:train:76]
Epoch: 30 | train_loss: 0.26717, val_loss: 0.47708

INFO 2023-02-11 11:56:06,379 [root:train.py:train:76]
Epoch: 40 | train_loss: 0.23027, val_loss: 0.45172

INFO 2023-02-11 11:56:06,436 [root:train.py:train:76]
Epoch: 50 | train_loss: 0.20675, val_loss: 0.43623

INFO 2023-02-11 11:56:06,493 [root:train.py:train:76]
Epoch: 60 | train_loss: 0.19047, val_loss: 0.42587

INFO 2023-02-11 11:56:06,549 [root:train.py:train:76]
Epoch: 70 | train_loss: 0.17923, val_loss: 0.41851

INFO 2023-02-11 11:56:06,607 [root:train.py:train:76]
Epoch: 80 | train_loss: 0.17120, val_loss: 0.41324

INFO 2023-02-11 11:56:06,664 [root:train.py:train:76]
Epoch: 90 | train_loss: 0.16496, val_loss: 0.40968

INFO 2023-02-11 11:56:06,721 [root:train.py:objective:134]
{
  "precision": 0.9116161616161617,
  "recall": 0.7569444444444444,
  "f1": 0.7915787915787916,
  "num_samples": 144.0
}

INFO 2023-02-11 11:56:06,736 [root:main.py:optimize:121]

Best value (f1): 0.8265063522446623

INFO 2023-02-11 11:56:06,737 [root:main.py:optimize:122]
Best hyperparameters: {
  "analyzer": "char_wb",
  "ngram_max_range": 4,
  "learning_rate": 0.11160741497018364,
  "power_t": 0.34894698040458927
}

INFO 2023-02-11 11:56:41,523 [root:main.py:train_model:65]
Run ID: 9c125ef8056e47efbdedb6f1f53a8a46

INFO 2023-02-11 11:56:41,651 [root:train.py:train:76]
Epoch: 00 | train_loss: 1.05181, val_loss: 1.09943

INFO 2023-02-11 11:56:41,707 [root:train.py:train:76]
Epoch: 10 | train_loss: 0.38984, val_loss: 0.54849

INFO 2023-02-11 11:56:41,764 [root:train.py:train:76]
Epoch: 20 | train_loss: 0.27286, val_loss: 0.46445

INFO 2023-02-11 11:56:41,819 [root:train.py:train:76]
Epoch: 30 | train_loss: 0.22266, val_loss: 0.43084

INFO 2023-02-11 11:56:41,875 [root:train.py:train:76]
Epoch: 40 | train_loss: 0.19608, val_loss: 0.41373

INFO 2023-02-11 11:56:41,931 [root:train.py:train:76]
Epoch: 50 | train_loss: 0.17979, val_loss: 0.40395

INFO 2023-02-11 11:56:41,986 [root:train.py:train:76]
Epoch: 60 | train_loss: 0.16880, val_loss: 0.39784

INFO 2023-02-11 11:56:42,040 [root:train.py:train:76]
Epoch: 70 | train_loss: 0.16168, val_loss: 0.39334

INFO 2023-02-11 11:56:42,096 [root:train.py:train:76]
Epoch: 80 | train_loss: 0.15694, val_loss: 0.39037

INFO 2023-02-11 11:56:42,152 [root:train.py:train:76]
Epoch: 90 | train_loss: 0.15315, val_loss: 0.38886

INFO 2023-02-11 11:56:42,215 [root:main.py:train_model:68]
{
  "overall": {
    "precision": 0.9178004535147392,
    "recall": 0.7986111111111112,
    "f1": 0.8265063522446623,
    "num_samples": 144.0
  },
  "class": {
    "computer-vision": {
      "precision": 1.0,
      "recall": 0.7037037037037037,
      "f1": 0.8260869565217391,
      "num_samples": 54.0
    },
    "mlops": {
      "precision": 1.0,
      "recall": 0.75,
      "f1": 0.8571428571428571,
      "num_samples": 12.0
    },
    "natural-language-processing": {
      "precision": 1.0,
      "recall": 0.8275862068965517,
      "f1": 0.9056603773584906,
      "num_samples": 58.0
    },
    "other": {
      "precision": 0.40816326530612246,
      "recall": 1.0,
      "f1": 0.5797101449275363,
      "num_samples": 20.0
    }
  },
  "slices": {
    "nlp_cnn": {
      "precision": 1.0,
      "recall": 1.0,
      "f1": 1.0,
      "num_samples": 1
    },
    "short_text": {
      "precision": 0.8,
      "recall": 0.8,
      "f1": 0.8000000000000002,
      "num_samples": 5
    }
  }
}

INFO 2023-02-11 11:56:42,252 [root:file_util.py:copy_file:137]
copying /var/folders/6w/rwxjjfl12yg5rxnfzhg2rrp40000gn/T/tmp5beukjpj/args.json -> /Users/alokrajgupta/ml-web/mlops/stores/model/762791503755819567/9c125ef8056e47efbdedb6f1f53a8a46/artifacts

INFO 2023-02-11 11:56:42,253 [root:file_util.py:copy_file:137]
copying /var/folders/6w/rwxjjfl12yg5rxnfzhg2rrp40000gn/T/tmp5beukjpj/model.pkl -> /Users/alokrajgupta/ml-web/mlops/stores/model/762791503755819567/9c125ef8056e47efbdedb6f1f53a8a46/artifacts

INFO 2023-02-11 11:56:42,254 [root:file_util.py:copy_file:137]
copying /var/folders/6w/rwxjjfl12yg5rxnfzhg2rrp40000gn/T/tmp5beukjpj/performance.json -> /Users/alokrajgupta/ml-web/mlops/stores/model/762791503755819567/9c125ef8056e47efbdedb6f1f53a8a46/artifacts

INFO 2023-02-11 11:56:42,254 [root:file_util.py:copy_file:137]
copying /var/folders/6w/rwxjjfl12yg5rxnfzhg2rrp40000gn/T/tmp5beukjpj/label_encoder.json -> /Users/alokrajgupta/ml-web/mlops/stores/model/762791503755819567/9c125ef8056e47efbdedb6f1f53a8a46/artifacts

INFO 2023-02-11 11:56:42,255 [root:file_util.py:copy_file:137]
copying /var/folders/6w/rwxjjfl12yg5rxnfzhg2rrp40000gn/T/tmp5beukjpj/vectorizer.pkl -> /Users/alokrajgupta/ml-web/mlops/stores/model/762791503755819567/9c125ef8056e47efbdedb6f1f53a8a46/artifacts

INFO 2023-02-11 11:56:48,870 [root:main.py:elt_data:40]
✅ Saved data!

INFO 2023-02-11 11:56:49,085 [root:main.py:predict_tag:166]
[
  {
    "input_text": "Transfer learning with transformers for text classification.",
    "predicted_tag": "natural-language-processing"
  }
]

INFO 2023-02-11 12:07:41,522 [root:main.py:elt_data:40]
✅ Saved data!

INFO 2023-02-11 12:07:41,820 [root:main.py:predict_tag:166]
[
  {
    "input_text": "Transfer learning with transformers for text classification.",
    "predicted_tag": "natural-language-processing"
  }
]

INFO 2023-02-11 12:10:45,724 [root:main.py:elt_data:40]
✅ Saved data!

INFO 2023-02-11 12:10:45,810 [root:main.py:train_model:65]
Run ID: 9217eb47f0154813883703235808f553

INFO 2023-02-11 12:10:45,846 [root:train.py:train:76]
Epoch: 00 | train_loss: 0.65307, val_loss: 0.67170

INFO 2023-02-11 12:10:45,867 [root:main.py:train_model:68]
{
  "overall": {
    "precision": 0.8169312169312168,
    "recall": 0.8,
    "f1": 0.7923444976076556,
    "num_samples": 30.0
  },
  "class": {
    "natural-language-processing": {
      "precision": 0.8888888888888888,
      "recall": 0.6153846153846154,
      "f1": 0.7272727272727274,
      "num_samples": 13.0
    },
    "other": {
      "precision": 0.7619047619047619,
      "recall": 0.9411764705882353,
      "f1": 0.8421052631578947,
      "num_samples": 17.0
    }
  },
  "slices": {}
}

INFO 2023-02-11 12:10:45,891 [root:file_util.py:copy_file:137]
copying /var/folders/6w/rwxjjfl12yg5rxnfzhg2rrp40000gn/T/tmp4bpr55fq/args.json -> /Users/alokrajgupta/ml-web/mlops/stores/model/767964735007127876/9217eb47f0154813883703235808f553/artifacts

INFO 2023-02-11 12:10:45,892 [root:file_util.py:copy_file:137]
copying /var/folders/6w/rwxjjfl12yg5rxnfzhg2rrp40000gn/T/tmp4bpr55fq/model.pkl -> /Users/alokrajgupta/ml-web/mlops/stores/model/767964735007127876/9217eb47f0154813883703235808f553/artifacts

INFO 2023-02-11 12:10:45,893 [root:file_util.py:copy_file:137]
copying /var/folders/6w/rwxjjfl12yg5rxnfzhg2rrp40000gn/T/tmp4bpr55fq/performance.json -> /Users/alokrajgupta/ml-web/mlops/stores/model/767964735007127876/9217eb47f0154813883703235808f553/artifacts

INFO 2023-02-11 12:10:45,894 [root:file_util.py:copy_file:137]
copying /var/folders/6w/rwxjjfl12yg5rxnfzhg2rrp40000gn/T/tmp4bpr55fq/label_encoder.json -> /Users/alokrajgupta/ml-web/mlops/stores/model/767964735007127876/9217eb47f0154813883703235808f553/artifacts

INFO 2023-02-11 12:10:45,896 [root:file_util.py:copy_file:137]
copying /var/folders/6w/rwxjjfl12yg5rxnfzhg2rrp40000gn/T/tmp4bpr55fq/vectorizer.pkl -> /Users/alokrajgupta/ml-web/mlops/stores/model/767964735007127876/9217eb47f0154813883703235808f553/artifacts

INFO 2023-02-11 12:10:45,929 [root:train.py:train:76]
Epoch: 00 | train_loss: 0.54013, val_loss: 0.64684

INFO 2023-02-11 12:10:45,945 [root:train.py:objective:134]
{
  "precision": 0.8920634920634921,
  "recall": 0.8666666666666667,
  "f1": 0.8615629984051036,
  "num_samples": 30.0
}

INFO 2023-02-11 12:10:45,960 [root:main.py:optimize:121]

Best value (f1): 0.8615629984051036

INFO 2023-02-11 12:10:45,961 [root:main.py:optimize:122]
Best hyperparameters: {
  "analyzer": "word",
  "ngram_max_range": 4,
  "learning_rate": 0.5519533764303033,
  "power_t": 0.2774599341445897
}

INFO 2023-02-11 12:10:46,037 [root:main.py:predict_tag:166]
[
  {
    "input_text": "Transfer learning with transformers for text classification.",
    "predicted_tag": "natural-language-processing"
  }
]

INFO 2023-02-11 12:10:54,226 [root:main.py:elt_data:40]
✅ Saved data!

INFO 2023-02-11 12:10:54,296 [root:main.py:train_model:65]
Run ID: 15b5a60a3ed34495a70d4ece566e0f29

INFO 2023-02-11 12:10:54,316 [root:train.py:train:76]
Epoch: 00 | train_loss: 0.54013, val_loss: 0.64684

INFO 2023-02-11 12:10:54,331 [root:main.py:train_model:68]
{
  "overall": {
    "precision": 0.8920634920634921,
    "recall": 0.8666666666666667,
    "f1": 0.8615629984051036,
    "num_samples": 30.0
  },
  "class": {
    "natural-language-processing": {
      "precision": 1.0,
      "recall": 0.6923076923076923,
      "f1": 0.8181818181818181,
      "num_samples": 13.0
    },
    "other": {
      "precision": 0.8095238095238095,
      "recall": 1.0,
      "f1": 0.8947368421052632,
      "num_samples": 17.0
    }
  },
  "slices": {}
}

INFO 2023-02-11 12:10:54,350 [root:file_util.py:copy_file:137]
copying /var/folders/6w/rwxjjfl12yg5rxnfzhg2rrp40000gn/T/tmpusgy0mvi/args.json -> /Users/alokrajgupta/ml-web/mlops/stores/model/401820454495517731/15b5a60a3ed34495a70d4ece566e0f29/artifacts

INFO 2023-02-11 12:10:54,351 [root:file_util.py:copy_file:137]
copying /var/folders/6w/rwxjjfl12yg5rxnfzhg2rrp40000gn/T/tmpusgy0mvi/model.pkl -> /Users/alokrajgupta/ml-web/mlops/stores/model/401820454495517731/15b5a60a3ed34495a70d4ece566e0f29/artifacts

INFO 2023-02-11 12:10:54,351 [root:file_util.py:copy_file:137]
copying /var/folders/6w/rwxjjfl12yg5rxnfzhg2rrp40000gn/T/tmpusgy0mvi/performance.json -> /Users/alokrajgupta/ml-web/mlops/stores/model/401820454495517731/15b5a60a3ed34495a70d4ece566e0f29/artifacts

INFO 2023-02-11 12:10:54,352 [root:file_util.py:copy_file:137]
copying /var/folders/6w/rwxjjfl12yg5rxnfzhg2rrp40000gn/T/tmpusgy0mvi/label_encoder.json -> /Users/alokrajgupta/ml-web/mlops/stores/model/401820454495517731/15b5a60a3ed34495a70d4ece566e0f29/artifacts

INFO 2023-02-11 12:10:54,353 [root:file_util.py:copy_file:137]
copying /var/folders/6w/rwxjjfl12yg5rxnfzhg2rrp40000gn/T/tmpusgy0mvi/vectorizer.pkl -> /Users/alokrajgupta/ml-web/mlops/stores/model/401820454495517731/15b5a60a3ed34495a70d4ece566e0f29/artifacts

INFO 2023-02-11 12:10:54,443 [root:train.py:train:76]
Epoch: 00 | train_loss: 0.65390, val_loss: 0.67174

INFO 2023-02-11 12:10:54,456 [root:train.py:objective:134]
{
  "precision": 0.8347222222222223,
  "recall": 0.7666666666666667,
  "f1": 0.7436029097133077,
  "num_samples": 30.0
}

INFO 2023-02-11 12:10:54,469 [root:main.py:optimize:121]

Best value (f1): 0.7436029097133077

INFO 2023-02-11 12:10:54,470 [root:main.py:optimize:122]
Best hyperparameters: {
  "analyzer": "char",
  "ngram_max_range": 8,
  "learning_rate": 0.09113087931360742,
  "power_t": 0.43883666582542713
}

INFO 2023-02-11 12:10:54,507 [root:main.py:predict_tag:166]
[
  {
    "input_text": "Transfer learning with transformers for text classification.",
    "predicted_tag": "natural-language-processing"
  }
]

INFO 2023-02-11 15:57:13,291 [root:main.py:elt_data:40]
✅ Saved data!

INFO 2023-02-11 15:57:13,343 [root:main.py:predict_tag:166]
[
  {
    "input_text": "Transfer learning with transformers for text classification.",
    "predicted_tag": "natural-language-processing"
  }
]

